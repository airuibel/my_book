{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hadoop 启动\n",
    "\n",
    "```shell\n",
    "/root/bigdata/hadoop/sbin/start-all.sh\n",
    "```\n",
    "\n",
    "\n",
    "## hive 的启动\n",
    "\n",
    "- 1.启动hdfs,yarn\n",
    "\n",
    "```shell\n",
    "/root/bigdata/hadoop/sbin/start-all.sh\n",
    "```\n",
    "- 2.启动docker,mysql\n",
    "```shell\n",
    "docker ps\n",
    "docker container start mysql\n",
    "docker exec -it mysql bash\n",
    "```\n",
    "- 3.启动hive的元数据服务\n",
    "```shell\n",
    "hive --service metastore&\n",
    "```\n",
    "\n",
    "- 4.启动hive\n",
    "```shell\n",
    "hive\n",
    "```\n",
    "\n",
    "## 启动flume\n",
    "```shell\n",
    "flume：/root/bigdata/flume/bin/flume-ng agent -n a1 -f test.conf\n",
    "```\n",
    "\n",
    "```\n",
    "-c conf:指定 flume 自身的配置文件所在目录\n",
    "\n",
    "-f conf/netcat-logger.conf:指定我们所描述的采集方案\n",
    "\n",
    "-n a1:指定我们这个 agent 的名字\n",
    "```\n",
    "\n",
    "\n",
    "## 启动kafka\n",
    "\n",
    "- 1、开启zookeeper,需要在一直在服务器端实时运行，以守护进程运行\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/zookeeper-server-start.sh -daemon /root/bigdata/kafka/config/zookeeper.properties\n",
    "```\n",
    "\n",
    "- 2、kafka的服务\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties\n",
    "```\n",
    "\n",
    "- 3、开启生产者与消费者\n",
    "\n",
    "- 1.创建topic命令\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n",
    "```\n",
    "\n",
    "\n",
    "    * 开启消息生产者\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/kafka-console-producer.sh --broker-list 192.168.19.137:9092 --sync --topic click-trace\n",
    "```\n",
    "    \n",
    "    *  开启消费者\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace\n",
    "```\n",
    "\n",
    "\n",
    "- 查看所有topic\n",
    "\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181\n",
    "```\n",
    "\n",
    "- 2.通过生产者发送消息\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n",
    "```\n",
    "\n",
    "- 3.通过消费者消费消息\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test\n",
    "-from-beginning\n",
    "从最开始生产队的数据开始消费\n",
    "```\n",
    "- 4.查看所有topic\n",
    "```shell\n",
    "/root/bigdata/kafka/bin/kafka-topics.sh --list --zookeeper 192.168.19.137:2181\n",
    "```\n",
    "\n",
    "## 数据采集模块\n",
    "- 启动及查看\n",
    "\n",
    "```shell\n",
    "root/bigdata/hadoop/sbin/start-dfs.sh\n",
    "root/bigdata/hadoop/sbin/start-yarn.sh\n",
    "```\n",
    "\n",
    "\n",
    "## hbase的启动\n",
    "- 1.启动hadoop的集群\n",
    "\n",
    "```shell\n",
    "/root/bigdata/hadoop/sbin/start-all.sh\n",
    "```\n",
    "- 2.启动hbase\n",
    "```shell\n",
    "/root/bigdata/hbase/bin/start-hbase.sh\n",
    "```\n",
    "\n",
    "- 3.输入hbase shell（进入shell命令行）\n",
    "\n",
    "\n",
    "## HappyBase操作HBase\n",
    "- 1.启动hbase\n",
    "```shell\n",
    "/root/bigdata/hbase/bin/start-hbase.sh\n",
    "```\n",
    "\n",
    "- 2.启动HBase thrift server\n",
    "```shell\n",
    "/root/bigdata/hbase/bin/hbase-daemon.sh start thrift\n",
    "```\n",
    "\n",
    "\n",
    "## 启动spark\n",
    "\n",
    "- 1.集群的方式启动\n",
    "\n",
    "```shell\n",
    "/root/bigdata/spark/sbin/start-all.sh\n",
    "```\n",
    "- 2.local模式的启动\n",
    "- 2.1.启动hive的元数据服务\n",
    "\n",
    "```shell\n",
    "hive --service metastore&\n",
    "```\n",
    "\n",
    "- 2.2.启动pyspark\n",
    "```shell\n",
    "/root/bigdata/spark/bin/pyspark\n",
    "```\n",
    "\n",
    "## 利用Spark Streaming实现WordCount\n",
    "\n",
    "- 需求：监听某个端口上的网络数据，实时统计出现的不同单词个数。\n",
    "\n",
    "- 1，需要安装一个nc工具：\n",
    "\n",
    "```shell\n",
    "yum install nc.x86_64\n",
    "```\n",
    "\n",
    "- 2，执行指令：\n",
    "```shell\n",
    "nc -lk 9999 -v\n",
    "```\n",
    "\n",
    "\n",
    "## jupyter 的远程启动\n",
    "\n",
    "```shell\n",
    "jupyter notebook --allow-root --ip=0.0.0.0 --port=8800\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "0\n",
      "168\n",
      "49152\n",
      "18\n",
      "12625920\n",
      "129\n",
      "3232240128\n",
      "3232240257\n"
     ]
    }
   ],
   "source": [
    "def ip_transform(ip):\n",
    "    ips = ip.split(\".\")\n",
    "    ip_num = 0\n",
    "    for i in ips:\n",
    "        print(int(i))\n",
    "        print(ip_num<<8)\n",
    "        ip_num = int(i) | ip_num << 8\n",
    "    return ip_num\n",
    "\n",
    "print(ip_transform(\"192.168.18.129\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2565"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 10\n",
    "5 | a<<8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
