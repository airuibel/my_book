{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "课程介绍\n",
    "\n",
    "1. 决策树 \n",
    "\n",
    "   1. 案例\n",
    "   2. api\n",
    "\n",
    "2. 集成学习\n",
    "\n",
    "   一堆小模型解决一个问题\n",
    "\n",
    "3. 聚类算法\n",
    "\n",
    "   无监督Kmean\n",
    "\n",
    "### 3.5 Tf-idf文本特征提取\n",
    "\n",
    "tf  - 词频\n",
    "\n",
    "idf- 逆向文档频率\n",
    "\n",
    "tf-idf-思想,一个词在一篇文章中出现的概率高,其他文章很少出现,这个词重要\n",
    "\n",
    "TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的**重要程度。**\n",
    "\n",
    "#### 3.5.1 公式\n",
    "\n",
    "词频(if): 指的是某一个给定的词语在该文件中出现的频率\n",
    "\n",
    "逆向文档频率（inverse document frequency，idf）**由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到**\n",
    "\n",
    "tf-idf=tf* idf\n",
    "\n",
    "api\n",
    "\n",
    "**sklearn.feature_extraction.text.TfidfVectorizer**\n",
    "\n",
    "transfer = TfidfVectorizer(stop_words=[])\n",
    "\n",
    "transfer.fit_transform()\n",
    "\n",
    "# 4.5 决策树算法api\n",
    "\n",
    "class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)\n",
    "\n",
    "- criterion\n",
    "  - 特征选择标准\n",
    "  - \"gini\"或者\"entropy\"，前者代表基尼系数，后者代表信息增益。一默认\"gini\"，即CART算法。\n",
    "- min_samples_split\n",
    "  - 内部节点再划分所需最小样本数\n",
    "  - 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。\n",
    "- min_samples_leaf\n",
    "  - 叶子节点最少样本数\n",
    "  - 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。\n",
    "- max_depth\n",
    "  - 决策树最大深度\n",
    "  - 决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间\n",
    "\n",
    "\n",
    "\n",
    "# 4.6 案例：泰坦尼克号乘客生存预测\n",
    "\n",
    "建模流程\n",
    "\n",
    "1. 获取数据\n",
    "\n",
    "2. 数据基本处理\n",
    "\n",
    "   2.1 缺失值处理\n",
    "\n",
    "   2.2 确定特征值目标值\n",
    "\n",
    "   2.3 切割数据\n",
    "\n",
    "3. 特征工程\n",
    "\n",
    "   特征提取\n",
    "\n",
    "4. 建立模型\n",
    "\n",
    "   决策树算法\n",
    "\n",
    "5. 模型评估\n",
    "\n",
    "## 4 决策树可视化\n",
    "\n",
    "sklearn.tree.export_graphviz(estimator,out_file = 路径, feature_names=[‘’,’’])\n",
    "\n",
    "\n",
    "\n",
    "# 集成学习\n",
    "\n",
    "有一堆的小分类器(小模型) 解决一个问题\n",
    "\n",
    "# 5.1 集成学习算法简介\n",
    "\n",
    "不是算法  解决问题的方案\n",
    "\n",
    "## 1 什么是集成学习\n",
    "\n",
    "1个模型->提升准确率->模型复杂(超级个体)   ==> 过拟合\n",
    "\n",
    "弱者联盟 : 多个小模型解决一个问题 能力变强 不容易过拟合\n",
    "\n",
    "**因此优于任何一个单分类的做出预测。**\n",
    "\n",
    "## 2 **复习：机器学习的两个核心任务**\n",
    "\n",
    "优化模型\n",
    "\n",
    "**泛化性能**:模型适用性\n",
    "\n",
    "任务一：**如何优化训练数据** —> 主要用于**解决欠拟合问题**\n",
    "\n",
    "任务二：**如何提升泛化性能** —> 主要用于**解决过拟合问题**\n",
    "\n",
    "## 3 集成学习\n",
    "\n",
    "1. bagging 过拟合问题  n个模型 投票 同一个问题 互相扼制变壮  \n",
    "2. boosting 欠拟合  分段拟合 \n",
    "\n",
    "最终结果:\n",
    "\n",
    " **只要单分类器的表现不太差，集成学习的结果总是要好于单分类器的**\n",
    "\n",
    "集成学习比较好\n",
    "\n",
    "# 5.2 Bagging\n",
    "\n",
    "解决 过拟合问题\n",
    "\n",
    "## 1 Bagging集成原理\n",
    "\n",
    "1. 采集不同子样本(n)\n",
    "2. n个子样本分别构建模型\n",
    "3. n个模型对一个问题平权投票\n",
    "\n",
    "## 2 随机森林构造过程\n",
    "\n",
    "bagging+决策树\n",
    "\n",
    "**随机森林是一个包含多个决策树的分类器**\n",
    "\n",
    "1. 选取m个样本数据(m<<M)\n",
    "2. 随机选取k个特征(k<K)\n",
    "3. 构建一个决策树\n",
    "4. 迭代1-3 步骤  n\n",
    "\n",
    "n 棵树构建一个森林    \n",
    "\n",
    "n树对一个问题 平权投票   票数多的作为最后结果\n",
    "\n",
    "m个样本需要有放回的随机抽样\n",
    "\n",
    "1.为什么要随机抽样训练集？　\n",
    "\n",
    "如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的\n",
    "\n",
    "2.为什么要有放回地抽样？\n",
    "\n",
    "无放回每个样本被抽取的概率就不同 \n",
    "\n",
    "## 3 随机森林api介绍\n",
    "\n",
    "- sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)\n",
    "  - n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200\n",
    "  - Criterion：string，可选（default =“gini”）分割特征的测量方法\n",
    "  - max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30\n",
    "  - max_features=\"auto”,每个决策树的最大特征数量\n",
    "    - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "    - If \"sqrt\", then `max_features=sqrt(n_features)`(same as \"auto\").\n",
    "    - If \"log2\", then `max_features=log2(n_features)`.\n",
    "    - If None, then `max_features=n_features`.\n",
    "  - bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样\n",
    "  - min_samples_split:节点划分最少样本数\n",
    "  - min_samples_leaf:叶子节点的最小样本数\n",
    "- 超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf\n",
    "\n",
    "## 5 bagging集成优点\n",
    "\n",
    "**Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习方法**\n",
    "\n",
    "**均可在原有算法上提高约2%左右的泛化正确率**\n",
    "\n",
    "**简单, 方便, 通用**\n",
    "\n",
    "# 5.3 Boosting\n",
    "\n",
    "解决  欠拟合\n",
    "\n",
    "## 1.boosting集成原理\n",
    "\n",
    "### 1.1 什么是boosting\n",
    "\n",
    "通过学习 从弱到强的过程---分段拟合 \n",
    "\n",
    "1. Adaboost\n",
    "2. GBDT\n",
    "3. XGBoost\n",
    "\n",
    "### 1.2 实现过程：Adaboost\n",
    "\n",
    "1. 注意力错误的数据上\n",
    "   1. 放大错误数据的权重\n",
    "   2. 缩小预测正确数据的权重\n",
    "\n",
    "1. 训练一个模型\n",
    "2. 根据模型结果 放大错误数据缩小正确数据\n",
    "3. 再训练一个模型\n",
    "4. 迭代1-3 \n",
    "5. 加权投票\n",
    "\n",
    "**如何确认投票权重？**\n",
    "\n",
    "**如何调整数据分布？**\n",
    "\n",
    "通过投票权重调整数据分布\n",
    "\n",
    "预测正确 缩小\n",
    "\n",
    "预测错误 放大\n",
    "\n",
    "**bagging集成与boosting集成的区别**\n",
    "\n",
    "1. 数据方面\n",
    "\n",
    "   Bagging 有放回随机采样\n",
    "\n",
    "   Boosting  根据前一轮的结果调整数据重要性\n",
    "\n",
    "2. 投票方面\n",
    "\n",
    "   Bagging 平权投票\n",
    "\n",
    "   Boosting 加权投票\n",
    "\n",
    "3. 学习顺序\n",
    "\n",
    "   Bagging  并行的 独立训练 互相没有任何关系\n",
    "\n",
    "   Boosting  串行 学习有先后顺序\n",
    "\n",
    "4. 主要作用\n",
    "\n",
    "   Bagging  过拟合问题\n",
    "\n",
    "   Boosting  欠拟合问题\n",
    "\n",
    "### 1.3 api介绍\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "## 2 GBDT(了解)\n",
    "\n",
    "梯度提升决策树(GBDT Gradient Boosting Decision Tree) **是一种迭代的决策树算法**，**该算法由多棵决策树组成，所有树的结论累加起来做最终答案。**\n",
    "\n",
    "**GBDT = 梯度下降 + Boosting + 决策树**\n",
    "\n",
    "### 2.1 梯度的概念(复习)\n",
    "\n",
    "导数 -微分\n",
    "\n",
    "### 2.4 GBDT主要执行思想\n",
    "\n",
    "1.使用梯度下降法优化代价函数；\n",
    "\n",
    "2. 使用一层决策树作为弱学习器，负梯度作为目标值；\n",
    "3. 3.利用boosting思想进行集成。\n",
    "\n",
    "## 3.XGBoost【了解】\n",
    "\n",
    "**XGBoost= 二阶泰勒展开+boosting+决策树+正则化**\n",
    "\n",
    "**二阶泰勒展开**: XGBoost对损失函数进行二阶泰勒展开，\n",
    "\n",
    "**正则化**：对损失函数+惩罚函数  避免过拟合\n",
    "\n",
    "# 聚类算法\n",
    "\n",
    "无监督学习  找到数据内部规律和结构的过程\n",
    "\n",
    "# 6.1 聚类算法简介\n",
    "\n",
    "## 1 认识聚类算法\n",
    "\n",
    "类别数???\n",
    "\n",
    "规则\n",
    "\n",
    "**使用不同的聚类准则，产生的聚类结果不同**。\n",
    "\n",
    "### 1.1 聚类算法在现实中的应用\n",
    "\n",
    "推荐系统\n",
    "\n",
    "用户画像，广告推荐，Data Segmentation，搜索引擎的流量推荐，恶意流量识别\n",
    "\n",
    "### 1.2 聚类算法的概念\n",
    "\n",
    "一种典型的**无监督**学习算法，主要用于将相似的样本自动归到一个类别中。\n",
    "\n",
    "相似度量:  欧式距离\n",
    "\n",
    "### 1.3 聚类算法与分类算法最大的区别\n",
    "\n",
    "聚类算法  无监督学习\n",
    "\n",
    "分类算法: 有监督学习\n",
    "\n",
    "# 6.2 聚类算法api初步使用\n",
    "\n",
    "sklearn.cluster.KMeans(n_clusters=8)\n",
    "\n",
    "n_clusters:开始的聚类中心数量\n",
    "\n",
    "- 整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。\n",
    "\n",
    "estimator = Kmeans(n_clusters=8)\n",
    "\n",
    "- estimator.fit(x)\n",
    "\n",
    "- estimator.predict(x)\n",
    "\n",
    "- estimator.fit_predict(x)\n",
    "\n",
    "  - 计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)\n",
    "\n",
    "  流程分析\n",
    "\n",
    "  \n",
    "\n",
    "# 6.3 聚类算法实现流程\n",
    "\n",
    "**k-means其实包含两层内容**\n",
    "\n",
    "K---中心个数 (聚成几个类)\n",
    "\n",
    "means(均值)\n",
    "\n",
    "## 1 k-means聚类步骤\n",
    "\n",
    "1. 随机找k个点作为中心点\n",
    "\n",
    "2. 计算所有样本与k个中心点的距离 贴标(离他最近)\n",
    "\n",
    "3. 计算每个群中的x均值 y的均值  作为新中心点\n",
    "\n",
    "4. 对比新的中心点与上次的中心点是否相同\n",
    "\n",
    "   1. 相同 停止迭代\n",
    "   2. 不同  新的中心点作为中心,迭代2-4\n",
    "\n",
    "   \n",
    "\n",
    "K  ????? \n",
    "\n",
    "找中心的过程????\n",
    "\n",
    "**K-Means一定会停下**\n",
    "\n",
    "1. 设定迭代次数\n",
    "2. 设定当前中心点与上一个中心点的差值小于阈值 停止迭代\n",
    "\n",
    "# 6.4 模型评估\n",
    "\n",
    "帮我们找到最优的K\n",
    "\n",
    "## 1 误差平方和(SSE \\The sum of squares due to error)：\n",
    "\n",
    "每个类中样本点与中心点的距离平方和\n",
    "\n",
    "SSE 越小越好\n",
    "\n",
    "初始中心找不好,容易陷入局部最优解 \n",
    "\n",
    "寻找最优初始中心?????\n",
    "\n",
    "## 2 **“肘”方法 (Elbow method)** — K值确定\n",
    "\n",
    "目标: 相对小的K值 达到较小的损失\n",
    "\n",
    "K  在肘的位置上 **下降率突然变缓时即认为是最佳的k值**。\n",
    "\n",
    "## 3 轮廓系数法（Silhouette Coefficient）\n",
    "\n",
    "a:  组内距离    越小越好\n",
    "\n",
    "b: 组之间距离   越大越好\n",
    "\n",
    "b =1  a=0    ==>好 \n",
    "\n",
    "s=(b-a)/max(a,b)=1/1=1   =>好\n",
    "\n",
    "a=1   b=0   ===>分类效果不好\n",
    "\n",
    "S=(b-a)/max(a,b)=-1/1=-1   ===>效果不好\n",
    "\n",
    "S:(-1,1)   越接近1 越好 \n",
    "\n",
    "​\t\t\t\t越接近-1  效果差\n",
    "\n",
    "\n",
    "\n",
    "## 4 CH系数（Calinski-Harabasz Index）\n",
    "\n",
    "CH需要达到的目的：\n",
    "\n",
    " **用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。**\n",
    "\n",
    "小的K 聚类,同时获得较好的聚类效果\n",
    "\n",
    "# 6.5 算法优化\n",
    "\n",
    "如何找到一个最优初始中心\n",
    "\n",
    "**k-means算法小结**\n",
    "\n",
    "优点:\n",
    "\n",
    " 1.原理简单（靠近中心点），实现容易\n",
    "\n",
    "2.聚类效果中上（依赖K的选择）\n",
    "\n",
    "3.空间复杂度o(N)，时间复杂度o(I*K*N)\n",
    "\n",
    "```\n",
    "N为样本点个数，K为中心点个数，I为迭代次数\n",
    "```\n",
    "\n",
    "**缺点：**\n",
    "\n",
    " 1.对离群点，噪声敏感 （中心点易偏移）mean\n",
    "\n",
    " 2.很难发现大小差别很大的簇及进行增量计算\n",
    "\n",
    "3. 如果初始中心选择不好,很容易陷入局部最优解\n",
    "\n",
    "   \n",
    "\n",
    "## 1 Canopy算法配合初始聚类\n",
    "\n",
    "找到最优初始中心\n",
    "\n",
    "### 1.2 Canopy算法的优缺点\n",
    "\n",
    "优点\n",
    "\n",
    "1.Kmeans对噪声抗干扰较弱，通过Canopy对比，将较小的NumPoint的Cluster直接去掉有利于抗干扰。==>比较稀疏的同心圆去掉\n",
    "\n",
    " 2.Canopy选择出来的每个Canopy的centerPoint作为K会更精确。\n",
    "\n",
    " 3.只是针对每个Canopy的内做Kmeans聚类，减少相似计算的数量。\n",
    "\n",
    "缺点：\n",
    "\n",
    " 1.算法中 T1、T2的确定问题 ，依旧可能落入局部最优解\n",
    "\n",
    "## 2 K-means++\n",
    "\n",
    "让初始中心尽可能远\n",
    "\n",
    "P 越大越好 \n",
    "\n",
    "kmeans++目的，让选择的质心尽可能的分散\n",
    "\n",
    "## 3 二分k-means\n",
    "\n",
    "- 1.所有点作为一个簇\n",
    "- 2.将该簇一分为二\n",
    "- 3.选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。\n",
    "- 4.以此进行下去，直到簇的数目等于用户给定的数目k为止\n",
    "\n",
    "## 4 k-medoids（k-中心聚类算法）\n",
    "\n",
    "1.对离群点，噪声敏感 （中心点易偏移）mean\n",
    "\n",
    "K-medoids中，将从当前cluster 中选取到其他所有（当前cluster中的）点的距离之和最小的点作为中心点。\n",
    "\n",
    "优点: 解决Kmean的缺点(中心点容易偏移)\n",
    "\n",
    "缺点: 计算量大(很明显)\n",
    "\n",
    "K-medoids只适用于小样本集\n",
    "\n",
    "当样本多的时候，少数几个噪音对k-means的质心影响也没有想象中的那么重，所以k-means的应用明显比k-medoids多\n",
    "\n",
    "## 5 Kernel k-means\n",
    "\n",
    "kernel k-means实际上，就是将每个样本进行一个投射到高维空间的处理\n",
    "\n",
    "## 6 ISODATA\n",
    "\n",
    "无固定K的对样本不断分裂和合并\n",
    "\n",
    "类别数目随着聚类过程而变化； 无固定K\n",
    "\n",
    "“合并”：（当聚类结果某一类中样本数太少，或两个类间的距离太近时）\n",
    "\n",
    "“分裂”：（当聚类结果中某一类的类内方差太大，将该类进行分裂）\n",
    "\n",
    "## 7 Mini Batch K-Means（了解）\n",
    "\n",
    "1. 随机抽取一小部分样本 计算质心\n",
    "\n",
    "2. 再抽取一部分 计算质心,求均值\n",
    "\n",
    "3. 不断迭代,直到质心稳定,或者达到指定的迭代次数  停止计算\n",
    "\n",
    "   \n",
    "\n",
    "# 6.6 特征降维\n",
    "\n",
    "1. 特征提取\n",
    "\n",
    "2. 特征预处理\n",
    "\n",
    "3. 特征降维\n",
    "\n",
    "   ## 1 降维\n",
    "\n",
    "   减少特征个数 得到\"不相关\"的主变量\n",
    "\n",
    "   \n",
    "\n",
    "相关特征(correlated feature)\n",
    "\n",
    "相对湿度与降雨量之间的相关\n",
    "\n",
    "可能存在共线性 去掉一个\n",
    "\n",
    "### 1.2 降维的两种方式\n",
    "\n",
    "1. 特征选择\n",
    "2. 主成分分析(PCA)\n",
    "\n",
    "## 2 特征选择\n",
    "\n",
    "### 2.2 方法\n",
    "\n",
    "Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联\n",
    "\n",
    "- **方差选择法：低方差特征过滤**\n",
    "- **相关系数**\n",
    "\n",
    "Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）\n",
    "\n",
    "- **决策树:信息熵、信息增益**\n",
    "- **正则化：L1、L2**\n",
    "- **深度学习：卷积等**\n",
    "- \n",
    "\n",
    "### 2.3 低方差特征过滤\n",
    "\n",
    "特征方差小：某个特征大多样本的值比较相近\n",
    "\n",
    "方差=0  值相等(常数)\n",
    "\n",
    "特征方差大：某个特征很多样本的值都有差别\n",
    "\n",
    "#### 2.3.1 API\n",
    "\n",
    "sklearn.feature_selection.VarianceThreshold(threshold = 0.0)\n",
    "\n",
    "1. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
